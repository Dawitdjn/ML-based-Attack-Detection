{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280ab246",
   "metadata": {},
   "source": [
    "### GWO Optimized XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88dc46",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import array\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt  \n",
    "from matplotlib import pyplot   \n",
    "import seaborn as sns  \n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder  \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer, classification_report  \n",
    "from sklearn.metrics import auc, f1_score, roc_curve, roc_auc_score  \n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdebdb9",
   "metadata": {},
   "source": [
    "### Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd89d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Desktop/full-cicids2017/MachineLearningCVE/'  \n",
    "\n",
    "ls = []\n",
    "\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        print(filename)\n",
    "        \n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        \n",
    "        df_ids2017 = pd.read_csv(file_path)\n",
    "        \n",
    "        ls.append(df_ids2017)\n",
    "        \n",
    "        print(f'Shape: {df_ids2017.shape}. Attack Type: {df_ids2017[' Label'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6396b8d5",
   "metadata": {},
   "source": [
    "### Pre-processing and other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017 = pd.concat(ls)\n",
    "df_ids2017.head(3)\n",
    "assert df_ids2017.shape[1] == 79 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017.columns = df_ids2017.columns.str.strip()\n",
    "print(\"original length:\", len(df_ids2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a08d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6added",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing missing values\n",
    "df_ids2017.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_ids2017.dropna(inplace=True)\n",
    "print(\"after droping null values, the length of df:\", len(df_ids2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f370e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b541f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017.Label.replace(\"Web.*\", \"Web Attack\", regex=True, inplace=True)\n",
    "df_ids2017.Label.replace(r'.*Patator$', \"Brute Force\", regex=True,inplace=True)\n",
    "df_ids2017.Label.replace([\"DoS GoldenEye\", \"DoS Hulk\", \"DoS Slowhttptest\", \"DoS slowloris\"], \"DoS\", inplace=True)\n",
    "df_ids2017.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592fc048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017.Label.replace(\"Web.*\", \"Web Attack\", regex=True, inplace=True)\n",
    "df_ids2017.Label.replace(r'.*Patator$', \"Brute Force\", regex=True,inplace=True)\n",
    "df_ids2017.Label.replace([\"DoS GoldenEye\", \"DoS Hulk\", \"DoS Slowhttptest\", \"DoS slowloris\"], \"DoS\", inplace=True)\n",
    "df_ids2017.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017['Label'].replace({'PortScan': 'attack', 'Web Attack':'attack',\n",
    "                            'Brute Force': 'attack',\n",
    "                            'DDoS':'attack' , 'Bot':'attack',\n",
    "                            'Infiltration':'attack',\n",
    "                            'DoS':'attack', 'Heartbleed':'attack'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e780bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids2017['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff695fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df_ids2017['Label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17918766",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_target = LabelEncoder()\n",
    "df_ids2017.Label =le_target.fit_transform(df_ids2017.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cbb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df_ids2017['Label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ae1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ids2017.drop('Label', axis=1)\n",
    "y = df_ids2017['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Balancing  \n",
    "rus = RandomUnderSampler(sampling_strategy={0: 1414698}, random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "smote = SMOTE(sampling_strategy={1: 1414698}, random_state=42)\n",
    "x_train_input, y_train_target = smote.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "# distribution of classes after balancing\n",
    "print(pd.Series(y_train_target).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07214bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "x_train_input, x_test_input, y_train_target, y_test_target = train_test_split(x_train_input,  y_train_target,test_size=0.30,random_state=42, stratify= y_train_target)\n",
    "print(\"after spliting the data:\\n\")\n",
    "print(\"training data input length:\", len(x_train_input))\n",
    "print(\"test data input  length:\", len(x_test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_input.shape, y_train_target.shape, x_test_input.shape, y_test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080325d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_cor = set()  \n",
    "    corr_matrix = x_train_input.corr().abs()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: \n",
    "                colname = corr_matrix.columns[i]  \n",
    "                col_cor.add(colname)\n",
    "    return col_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77de474",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features = correlation(x_train_input, 0.96) \n",
    "len(set(corr_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_input.drop(corr_features,axis=1, inplace=True)\n",
    "x_test_input.drop(corr_features,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_target.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_target.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_input_0, y_train_target_1 = y_train_target.value_counts()[0] / len(y_train_target) , y_train_target.value_counts()[1] / len(y_train_target)\n",
    "x_test_input_0, y_test_target_1 = y_test_target.value_counts()[0] / len(y_test_target), y_test_target.value_counts()[1] /len(y_test_target) \n",
    "\n",
    "print(\"In Train: there are {} % of class 0 and {} % of class 1\".format(x_train_input_0, y_train_target_1))\n",
    "print(\"In Test: there are {} % of class 0 and {} % of class 1\".format(x_test_input_0, y_test_target_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting target label distribution\n",
    "plt.figure()\n",
    "plt.title(\"class distribution of train and test dataset\")\n",
    "y_train_target.value_counts().plot(kind=\"bar\", color='c', label=\"y_train_target\")\n",
    "y_test_target.value_counts().plot(kind=\"bar\", color='blue', label=\"y_test_targe\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values after pre-processing\n",
    "x_train_input.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_input.isnull().sum().sum(), x_test_input.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_input.dtypes.value_counts()\n",
    "x_train_input.describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "scaler_t = MinMaxScaler()\n",
    "x_train_input_SC = scaler_t.fit_transform(x_train_input)\n",
    "x_test_input_SC = scaler_t.transform(x_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_input_SC.max().round()\n",
    "x_train_input_SC.min().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ecec41",
   "metadata": {},
   "source": [
    "### Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function definition\n",
    "def custom_loss(inputs, outputs, encoder_weights, decoder_weights, delta=1.0, alpha=0.001, beta=0.001, gamma=0.005, epsilon=1e-10):\n",
    "    term1 = tf.reduce_sum((delta * tf.square(inputs - outputs)) / (delta + tf.square(inputs - outputs)))\n",
    "    term2 = tf.reduce_sum(tf.square(inputs - outputs) / (tf.square(inputs - outputs) + delta))\n",
    "    els_loss = term1 + term2 \n",
    "\n",
    "    encoder_layer_weights = encoder_weights[0]\n",
    "    decoder_layer_weights = decoder_weights[0]\n",
    "\n",
    "    sparse_reg_term = alpha * tf.reduce_sum(tf.abs(encoder_layer_weights)) + alpha * tf.reduce_sum(tf.abs(decoder_layer_weights))\n",
    "\n",
    "    l2_reg_term = 0.5 * beta * tf.reduce_sum([tf.nn.l2_loss(w) for w in encoder_layer_weights]) + 0.5 * beta * tf.reduce_sum([tf.nn.l2_loss(w) for w in decoder_layer_weights])\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = els_loss + sparse_reg_term + l2_reg_term\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def custom_loss_wrapper(encoder_weights, decoder_weights, delta=1.0, alpha=0.001, beta=0.001):\n",
    "    def loss(inputs, outputs):\n",
    "        return custom_loss(inputs, outputs, encoder_weights, decoder_weights, delta, alpha, beta)\n",
    "    return loss\n",
    "\n",
    "def create_autoencoder(input_dim, encoding_dim, l1_reg, l2_reg, learning_rate):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(64, activation='relu', activity_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(input_layer)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "    \n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dense(64, activation='relu')(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "    encoder_weights = [layer.trainable_weights for layer in autoencoder.layers if 'dense' in layer.name and layer.output_shape == (None, encoding_dim)]\n",
    "    decoder_weights = [layer.trainable_weights for layer in autoencoder.layers if 'dense' in layer.name and layer.output_shape == (None, input_dim)]\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    autoencoder.compile(optimizer=optimizer, loss=custom_loss_wrapper(encoder_weights, decoder_weights))\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "def random_search(X_train_input_SC, hyperparameter_ranges, n_iter):\n",
    "    best_params = None\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        config = {param: np.random.choice(options) for param, options in hyperparameter_ranges.items()}\n",
    "        model, _ = create_autoencoder(input_dim=X_train_input_SC.shape[1], encoding_dim=config['encoding_dim'],\n",
    "                                   l1_reg=config['l1_reg'], l2_reg=config['l2_reg'], learning_rate=config['learning_rate'])\n",
    "        history = model.fit(X_train_input_SC, X_train_input_SC, epochs=config['num_epochs'],\n",
    "                            batch_size=config['batch_size'], verbose=0, validation_split=0.3)\n",
    "        mse = history.history['val_loss'][-1]\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_params = config\n",
    "\n",
    "    return best_params, best_mse\n",
    "\n",
    "# Hyperparameter search space\n",
    "hyperparameter_ranges = {\n",
    "    'batch_size': [32, 64, 128, 256, 1024],  \n",
    "    'l1_reg': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'l2_reg': [0.1, 0.01, 0.001, 0.0001], \n",
    "    'encoding_dim': [16], \n",
    "    'num_epochs': [50, 75, 100, 150, 200],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01, 0.02] \n",
    "}\n",
    "\n",
    "best_params, best_mse = random_search(x_train_input_SC, hyperparameter_ranges, n_iter=5)\n",
    "\n",
    "print(\"Best Configuration:\", best_params)\n",
    "print(\"Best MSE:\", best_mse)\n",
    "\n",
    "best_autoencoder, best_encoder = create_autoencoder(input_dim=x_train_input_SC.shape[1], encoding_dim=best_params['encoding_dim'],\n",
    "                                                    l1_reg=best_params['l1_reg'], l2_reg=best_params['l2_reg'], learning_rate=best_params['learning_rate'])\n",
    "\n",
    "history = best_autoencoder.fit(x_train_input_SC, x_train_input_SC, epochs=best_params['num_epochs'],\n",
    "                                batch_size=best_params['batch_size'], verbose=0, shuffle=True, validation_split=0.3)\n",
    "\n",
    "encoded_features = best_encoder.predict(x_test_input_SC)\n",
    "print(\"Encoded Features Shape:\", encoded_features.shape)\n",
    "\n",
    "X_Mae = best_autoencoder.predict(x_test_input_SC)\n",
    "mae_test = np.mean(np.abs(x_test_input_SC - X_Mae))\n",
    "print(\"Test MAE:\", mae_test)\n",
    "\n",
    "mse_test = best_autoencoder.evaluate(x_test_input_SC, x_test_input_SC, verbose=0)\n",
    "print(\"Test MSE:\", mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df208be",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features_train_EDA = best_autoencoder.predict(x_train_input_SC)\n",
    "encoded_features_test_EDA = best_autoencoder.predict(x_test_input_SC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(encoded_features_train_EDA[:, 1], encoded_features_train_EDA[:, 0], c=y_train_target, cmap='viridis')\n",
    "plt.colorbar(scatter, label='Class')\n",
    "plt.title('Encoded Features Visualization')\n",
    "plt.xlabel('Encoded Feature 1')\n",
    "plt.ylabel('Encoded Feature 2')\n",
    "#plt.savefig('FeE_Vclass2_01.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bac08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error = mean_absolute_error(x_train_input_SC, encoded_features_train_EDA, multioutput='raw_values')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_error, bins=50, density=True, alpha=0.7, color='blue')\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error (MAE)')\n",
    "plt.ylabel('Frequency')\n",
    "#plt.savefig('MAE_Encode_Recon_Error.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error = mean_squared_error(x_train_input_SC, encoded_features_train_EDA, multioutput='raw_values')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_error, bins=50, density=True, alpha=0.7, color='blue')\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Frequency')\n",
    "#plt.savefig('MSE_Encode_REcon_Error.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c99c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features_train_ENC = best_encoder.predict(x_train_input_SC)\n",
    "encoded_features_test_ENC = best_encoder.predict(x_test_input_SC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(encoded_features_train_ENC[:, 1], encoded_features_train_ENC[:, 0], c=y_train_target, cmap='viridis')\n",
    "plt.colorbar(scatter, label='Class')\n",
    "plt.title('Encoded Features Visualization')\n",
    "plt.xlabel('Encoded Feature 1')\n",
    "plt.ylabel('Encoded Feature 2')\n",
    "#plt.savefig('CICIDS2017_COMP_16_FeE_Vclass2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bef33f2",
   "metadata": {},
   "source": [
    "### XGBoost hyperparameter Optimization(GWO) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16fb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(hyperparameters):\n",
    "    xgb_model = xgb.XGBClassifier(**hyperparameters)\n",
    "    xgb_model.fit(encoded_features_train_ENC, y_train_target)\n",
    "    predictions = xgb_model.predict(encoded_features_test_ENC)\n",
    "    accuracy = accuracy_score(y_test_target, predictions)\n",
    "    return accuracy\n",
    "\n",
    "def initialize_population(population_size):\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        hyperparameters = {\n",
    "            'learning_rate': np.random.uniform(0.01, 0.3),\n",
    "            'max_depth': np.random.randint(4, 20),\n",
    "            'subsample': np.random.uniform(0.6, 1.0),\n",
    "            'min_child_weight': np.random.uniform(0.5, 2.0),\n",
    "            'n_estimators': np.random.randint(100, 300),\n",
    "            'max_leaf_nodes': np.random.randint(1, 9),\n",
    "            'colsample_bytree': np.random.uniform(0.5, 0.9),\n",
    "            'gamma': 0.1,\n",
    "            'scale_pos_weight':np.random.uniform(1, 20),\n",
    "            'seed': 42\n",
    "        }\n",
    "        population.append(hyperparameters)\n",
    "    return population\n",
    "\n",
    "def update_wolf_positions(population, alpha, beta, delta, a, A, C):\n",
    "    for wolf in population:\n",
    "        for key in wolf:\n",
    "            if key in alpha:\n",
    "                D_alpha = abs(C * alpha[key] - wolf[key])\n",
    "                X1 = alpha[key] - A * D_alpha\n",
    "                wolf[key] = X1\n",
    "            elif key in beta:\n",
    "                D_beta = abs(C * beta[key] - wolf[key])\n",
    "                X2 = beta[key] - A * D_beta\n",
    "                wolf[key] = X2\n",
    "            elif key in delta:\n",
    "                D_delta = abs(C * delta[key] - wolf[key])\n",
    "                X3 = delta[key] - A * D_delta\n",
    "                wolf[key] = X3\n",
    "            else:\n",
    "                choice = random.choice([alpha, beta, delta])\n",
    "                D_choice = abs(C * choice[key] - wolf[key])\n",
    "                X_choice = choice[key] - A * D_choice\n",
    "                wolf[key] = X_choice\n",
    "\n",
    "def evaluate_fitness(population):\n",
    "    fitness_scores = []\n",
    "    for wolf in population:\n",
    "        fitness_scores.append(objective_function(wolf))\n",
    "    return fitness_scores\n",
    "\n",
    "# Termination criteria\n",
    "max_iterations = 50 \n",
    "convergence_threshold = 1e-6 # to 1e-4\n",
    "max_consecutive_iterations = 5\n",
    "\n",
    "# Initialize best_params\n",
    "best_hyperparameters = {\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.5,\n",
    "    'n_estimators': 100,\n",
    "    'min_child_weight': 1.0,\n",
    "    'scale_pos_weight':1,\n",
    "    'gamma': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'max_leaf_nodes': 5,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_fitness = objective_function(best_hyperparameters)\n",
    "consecutive_iterations = 5 #3,7,10\n",
    "\n",
    "# Main GWO optimization \n",
    "for iteration in range(max_iterations):\n",
    "    # Initialize alpha, beta, delta wolves\n",
    "    alpha, beta, delta = None, None, None\n",
    "    population = initialize_population(population_size=50)  \n",
    "    \n",
    "    a = 2.0\n",
    "    A = 2 * a * (1 - (iteration / max_iterations))  \n",
    "    \n",
    "    fitness_scores = evaluate_fitness(population)\n",
    "    alpha_idx = fitness_scores.index(min(fitness_scores))\n",
    "    alpha = population[alpha_idx].copy()\n",
    "    beta_idx = fitness_scores.index(sorted(fitness_scores)[1])\n",
    "    beta = population[beta_idx].copy()\n",
    "    delta_idx = fitness_scores.index(sorted(fitness_scores)[2])\n",
    "    delta = population[delta_idx].copy()\n",
    "    \n",
    "    update_wolf_positions(population, alpha, beta, delta, a, A, 2 * random.random() - 1)\n",
    "    \n",
    "    best_hyperparameters = alpha  \n",
    "    \n",
    "    current_fitness = objective_function(best_hyperparameters)\n",
    "    \n",
    "    if current_fitness > best_fitness:\n",
    "        best_fitness = current_fitness\n",
    "        consecutive_iterations = 0\n",
    "    else:\n",
    "        consecutive_iterations += 1\n",
    "    \n",
    "    if consecutive_iterations >= max_consecutive_iterations:\n",
    "        print(\"Convergence criteria met. Terminating optimization.\")\n",
    "        break\n",
    "    \n",
    "    if iteration == max_iterations - 1:\n",
    "        print(\"Maximum iterations reached. Terminating optimization.\")\n",
    "\n",
    "print(\"Final Hyperparameters:\", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2802c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = xgb.XGBClassifier(n_estimators=150, learning_rate = 0.04475, max_depth = 8, min_child_weight= 0.8954022553993488, subsample= 0.7513841858486106, \n",
    "                        max_leaf_nodes = 5, gamma = 0.1, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d5233",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_classifier.fit(encoded_features_train_ENC, y_train_target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353de684",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_predxgp = xgb_classifier.predict(encoded_features_test_ENC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the confusion matrix heatmap function\n",
    "def confusionMatrixHeatMap(cm, title):\n",
    "    \n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_percentage = cm / cm_sum * 100\n",
    "\n",
    "    labels = np.array([[\"{}\\n{:.3f}%\".format(value, percentage) for value, percentage in zip(row, percentage_row)]\n",
    "                       for row, percentage_row in zip(cm, cm_percentage)])\n",
    "\n",
    "    # Categories\n",
    "    categories = ['Benign', 'Malicious']\n",
    "\n",
    "    max_value = np.max(cm)  \n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', cbar=True,\n",
    "                annot_kws={\"size\": 14}, linewidths=2, linecolor='black',\n",
    "                xticklabels=categories, yticklabels=categories, vmin=0, vmax=max_value)\n",
    "    # Set labels, title, and axis ticks\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(title)\n",
    "\n",
    "    #plt.savefig('confusion_matrix_GWO_xgboost.png')\n",
    "    plt.show()\n",
    "\n",
    "print(\"XGBoost\")\n",
    "\n",
    "# Generate report\n",
    "cm = confusion_matrix(y_test_target, y_predxgp )\n",
    "cr = classification_report(y_test_target, y_predxgp )\n",
    "auc = roc_auc_score(y_test_target, y_predxgp )\n",
    "\n",
    "# calculate accuracy and misclassification rate\n",
    "total_samples = len(y_test_target)\n",
    "correct_predictions = sum(y_test_target == y_predxgp)\n",
    "incorrect_predictions = total_samples - correct_predictions\n",
    "accuracy = correct_predictions / total_samples\n",
    "misclassification_rate = incorrect_predictions / total_samples\n",
    "\n",
    "plt.text(0.5, -0.1, f'\\n\\n\\nAccuracy: {accuracy:.4f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "plt.text(0.5, -0.2, f'\\nMisclassification Rate: {misclassification_rate:.4f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Performance Matrix:\")\n",
    "print(cr)\n",
    "\n",
    "print(\"AUC:\", auc)\n",
    "confusionMatrixHeatMap(cm, title=\"GWO-Optimized XGBoost\")\n",
    "#plt.savefig('CICIDS2017_CM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_target, y_predxgp)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.text(0.5, -0.1, f'\\n\\n\\nAccuracy: {accuracy:.4f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "plt.text(0.5, -0.2, f'\\nMisclassification Rate: {misclassification_rate:.4f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "#plt.savefig('ROC_OPXGB_.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Validation Accuracy\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=150, learning_rate = 0.04475, max_depth = 8, min_child_weight= 0.8954022553993488, subsample= 0.7513841858486106, \n",
    "                        max_leaf_nodes = 5, gamma = 0.1, seed = 42)\n",
    "\n",
    "# Fit the model with evaluation sets\n",
    "evals = [(encoded_features_train_ENC, encoded_y_train), (encoded_features_test_ENC, encoded_y_test)]\n",
    "xgb_classifier.fit(encoded_features_train_ENC, encoded_y_train, eval_metric=[\"error\", \"logloss\"], eval_set=evals, verbose=True)\n",
    "\n",
    "results = xgb_classifier.evals_result()\n",
    "\n",
    "train_accuracy = 1 - np.array(results[\"validation_0\"][\"error\"])\n",
    "val_accuracy = 1 - np.array(results[\"validation_1\"][\"error\"])\n",
    "\n",
    "epochs = range(1, len(train_accuracy) + 1)\n",
    "plt.plot(epochs, train_accuracy, label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_accuracy, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy over Epochs\")\n",
    "plt.ylim([0, 1.0])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "Xgbb_pred = xgb_classifier.predict(encoded_features_test_ENC)\n",
    "accuracy = accuracy_score(encoded_y_test, Xgbb_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "cr = classification_report(encoded_y_test, Xgbb_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1effe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
